{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "\n",
    "import sys                              # only required for next line\n",
    "sys.path.append('/data/Simone/packages')  # add path to own package\n",
    "import tools                          # load own package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'name': 'Stoat',                                    # codename of the model - used for output path and filenames\n",
    "    'train_from': 'Stoat_V59',                        # either: 'imagenet' or full model name such as 'Stoat_V4'\n",
    "    'input_size': 299,\n",
    "    'pred_dataset': 'kiel_clean_1471fold_000_valid',  # which dataset to predict\n",
    "    'col_cl_mapping': 'cl_15_w404',                  # which column mapping to use between real classes and target classes\n",
    "    'col_class': 'cl_manual',                           # column name in data .csv with class information\n",
    "    'col_image': 'filename' ,                            # column name in data .csv with filename of images (no path)\n",
    "    'col_id': 'defect_id'\n",
    "}\n",
    "\n",
    "path = {\n",
    "    'models': '/data/Simone/models/',                                # directory where models are saved\n",
    "\n",
    "    'csv_train': '/data/Simone/split_by_wafer/train.csv',   # path to .csv with training data (to be used with flow_from_dataframe Keras generator)\n",
    "    'csv_valid': '/data/Simone/split_by_wafer/valid.csv',   # path to .csv with validation data (to be used with flow_from_dataframe Keras generator)\n",
    "    'csv_test': '/data/Simone/split_by_wafer/test.csv',\n",
    "    'csv_predict': '/data/DefectDensity/ostsee/kiel/kiel_clean_1471fold_000_valid.csv', # path to csv or excel with data frame to images\n",
    "    'csv_meta': '/data/DefectDensity/ostsee/kiel_clean.xlsx',                            # path to csv or excel with metadata to enrich predictions\n",
    "    'csv_context': '/data/DefectDensity/contextdata/kiel_contextdata.csv',\n",
    "    \n",
    "    'data_train':'/images/kiel/' ,                              # path to image directory\n",
    "    'data_predict': '/images/kiel/',                            # path to image directory\n",
    "\n",
    "    'arch_overview': '/data/yury/models/model_architectures.xlsx', # path to excel with architecture overview\n",
    "    'class_map': '/data/DefectDensity/DefectClassMap_Metal.xlsx'    # path to excel with mapping of classes between\n",
    "}\n",
    "\n",
    "hyper = {\n",
    "    'n_epochs_max': 50,                # maximum number of epochs for training, only reached if early stopping doesn't trigger\n",
    "    'lr_min': 1e-5,                     # minimum value for lr search\n",
    "    'lr_max': 1e-0,                     # maximum value for lr search\n",
    "    'n_lr_search': 100,                 # how many steps to take in the lr search space (exponential by default)\n",
    "    'lr_decay': 0,                      # learing rate decay parameter, suggested to use 0 if lr reduction callback is used\n",
    "    'patience_stop': 10,                # epochs without improvement to wait for early stopping. 20 is better for \"final\" models\n",
    "    'patience_lr': 6,                   # epochs without improvement to wait until learning rate reduction is triggered, 2 reductions within patience_stop occur\n",
    "    'lr_factor': 0.33,                  # factor by which learning rate is multiplied when lr reduction is triggered. good experience with factor 3 -> 2 reductions lead to 1 order of magnitude\n",
    "    'save_best_epoch_only': True,       # flag if only best epoch is saved or all epochs, default True\n",
    "    'dropout': False,                   # flag if Mustilidae dropout layer should be included or not. Not implemented for Beaver\n",
    "    'n_threshold_steps': 50,            # number of steps for softmax threshold simulation (O(n)) - default 20 equals 5% steps\n",
    "    'target_acc': 0.8,                  # target accuracy for volume simulation\n",
    "    'keras_preprocessing': True,        # use Keras preprocessing function or rescaling instead\n",
    "    'tensorboard': 'none',              # write output files for tensorboard. Possible values: 'minimal', 'full' and 'none' (or anything else)\n",
    "    'remove_to_404': False,             # Flag if remove-class images are actually supposed to be removed from prediction analysis. Does not influence training where remove is always removed\n",
    "    'meta_columns': [\"tech\",\"route\",\"op\",\"tool\"],  # which columns to add from csv_meta to enriched predictions                \n",
    "    'context_columns': [\"x(wafer)[um]\",\"y(wafer)[um]\",\"x(die)[um]\",\"y(die)[um]\",\"die_x\",\"die_y\"]\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE SOME HYPER-PARAMETERS FROM ARCHITECTURE OVERVIEW\n",
    "d = pd.read_excel(path['arch_overview'], sheet_name=0)\n",
    "hyper['batch_size'] = d.loc[d['codename']==cfg['name'],'batch_size'].values[0]\n",
    "hyper['threshold_layer'] = d.loc[d['codename']==cfg['name'],'threshold_layer'].values[0]\n",
    "hyper['lr_fb'] = d.loc[d['codename']==cfg['name'],'default_lr_fb'].values[0] # default learning rate if lr search is not used for frozen base model\n",
    "hyper['lr_pt'] = d.loc[d['codename']==cfg['name'],'default_lr_pt'].values[0] # default learning rate if lr search is not used for pretrained top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','U']\n",
    "n_classes = len(cl_names)\n",
    "\n",
    "d_map = pd.read_excel(path['class_map'], dtype=str)\n",
    "d_train = tools.helper.load_dataframe(path['csv_train'] , cfg , d_map)\n",
    "d_valid = tools.helper.load_dataframe(path['csv_valid'] , cfg , d_map)\n",
    "d_test = tools.helper.load_dataframe(path['csv_test'] , cfg , d_map)\n",
    "\n",
    "print(\"Training samples: \",len(d_train))\n",
    "print(\"Validation samples: \",len(d_valid))\n",
    "print(\"Test samples: \",len(d_test))\n",
    "print(\"Number of classes: \",n_classes)\n",
    "\n",
    "# Get routes in order to estimate die sizes\n",
    "train_routes = np.unique(d_train['route'])\n",
    "valid_routes = np.unique(d_valid['route'])\n",
    "test_routes = np.unique(d_test['route'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE TRAIN, VALID AND TEST DATA SUCH THAT IT CAN BE FED TO DATA-GENERATOR\n",
    "train_die_sizes = tools.helper.get_die_size_by_route_dict(d_train, train_routes, hyper)\n",
    "valid_die_sizes = tools.helper.get_die_size_by_route_dict(d_valid, valid_routes, hyper)\n",
    "test_die_sizes = tools.helper.get_die_size_by_route_dict(d_test, test_routes, hyper)\n",
    "train_context,train_labels,train_filenames = tools.helper.prepare_data(d_train,train_die_sizes,hyper,polar=False)\n",
    "valid_context,valid_labels,valid_filenames = tools.helper.prepare_data(d_valid,valid_die_sizes,hyper,polar=False)\n",
    "test_context,test_labels,test_filenames = tools.helper.prepare_data(d_test,test_die_sizes,hyper,polar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAIN, VALID AND TEST DATA-GENERATOR\n",
    "train_datagen = tools.helper.DataGenerator(img_files=train_filenames,context_info=train_context, labels=train_labels, batch_size=hyper['batch_size'] , n_classes=n_classes)\n",
    "val_datagen = tools.helper.DataGenerator(img_files=valid_filenames,context_info=valid_context, labels=valid_labels, batch_size=hyper['batch_size'], n_classes=n_classes,shuffle=False)\n",
    "test_datagen = tools.helper.DataGenerator(img_files=test_filenames,context_info=test_context, labels=test_labels, batch_size=hyper['batch_size'], n_classes=n_classes,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cl = tools.helper.summarize_train_valid_test_data(d_train,d_valid,d_test,cl_names)\n",
    "# dictionary with weight for loss function adjustment\n",
    "cl_weight_dict = dict(zip(d_cl['cl_id'], d_cl['inv_ratio_adj_train'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "m = tf.keras.models.load_model('/data/Simone/models/Stoat_V55.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCATENATE CONTEXT FEAT AND RESIZE LAST LAYER\n",
    "\n",
    "# creates new model without last layer\n",
    "m_base = tf.keras.models.Model(inputs=m.input, outputs=m.layers[-2].output) \n",
    "    \n",
    "input_context = tf.keras.Input(shape=(4,) , name='input_context')\n",
    "#y = tf.keras.layers.Dense(128, activation='tanh')(input_context)\n",
    "#y = tf.keras.layers.Dense(32, activation='tanh')(y)\n",
    "#y = tf.keras.models.Model(inputs=input_context, outputs=y)\n",
    "y = tf.keras.models.Model(inputs=input_context, outputs=input_context)\n",
    "\n",
    "# combine the outputs of the two branches\n",
    "combined = tf.keras.layers.concatenate([m_base.output , y.output]) \n",
    "\n",
    "combined = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "classification_layer = tf.keras.layers.Dense(n_classes, activation='softmax', name='classification')(combined)\n",
    "m_combined = tf.keras.models.Model(inputs=[m_base.input , y.input], outputs=classification_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_combined.compile(optimizer=tf.keras.optimizers.Adam(lr=hyper['lr_pt'], decay=hyper['lr_decay']),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive some hyper-parameters for training\n",
    "hyper['n_steps_train'] = int(np.ceil(train_datagen.__len__()/train_datagen.batch_size))\n",
    "hyper['n_steps_valid'] = int(np.ceil(val_datagen.__len__()/val_datagen.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"path_to_directory_where_to_save_best_model\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_weights_only=False, save_best_only=True, mode='max')\n",
    "# learning rate reduction when there is no improvement\n",
    "check_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=hyper['lr_factor'], min_delta=0.0001, patience=hyper['patience_lr'], verbose=1)\n",
    "# early stopping to avoid having to set a fixed number of epochs\n",
    "check_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta = 0.0001, patience=hyper['patience_stop'], verbose=1, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,check_lr,check_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = m_combined.fit_generator(train_datagen, \n",
    "                           steps_per_epoch=hyper['n_steps_train'], \n",
    "                           epochs=hyper['n_epochs_max'], \n",
    "                           class_weight = cl_weight_dict,\n",
    "                           callbacks=callbacks_list,\n",
    "                           validation_data=val_datagen, \n",
    "                           validation_steps=hyper['n_steps_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "tools.helper.model_history_analysis(model_trained, 'path_to_directory_where_to_save_training_history', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "best_model = tf.keras.models.load_model(filepath)\n",
    "# compile best model\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(lr=hyper['lr_pt'], decay=hyper['lr_decay']), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE SOFTMAX PROBABILITIES AND PREDICTIONS\n",
    "probabilities = best_model.predict_generator(test_datagen, steps=len(test_datagen), verbose=1)\n",
    "predictions = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE TRUE LABELS\n",
    "test_values = np.array([t for t in test_labels.values()])\n",
    "test_values = test_values[0:probabilities.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "tools.helper.plot_confusion_matrix(test_values , predictions , n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification score\n",
    "report = classification_report(test_values, predictions)\n",
    "report_df = tools.helper.report_to_df(report,'path_to_directory_where_to_save_classification_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns keras probabilities into a properly named pandas data frame\n",
    "classes = []\n",
    "\n",
    "for i in range(probabilities.shape[0]):\n",
    "    classes.append(tools.helper.assign_class(test_values[i]))\n",
    "    \n",
    "d_softmax = tools.helper.prepare_base_table(new_probabilities, cl_names,classes,test_filenames,'path_to_directory_where_to_save_softmax_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax threshold analysis \n",
    "hyper['threshold_range'] = np.array(range(hyper['n_threshold_steps']))/hyper['n_threshold_steps']\n",
    "cfg['n_img'] = d_softmax.shape[0]\n",
    "d_thresh, d_thresh_cl = tools.helper.simulate_softmax_thresholds(d_softmax,cfg,hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot softmax threshold simulation\n",
    "tools.helper.plot_accuracy_and_volume_vs_softmax(d_thresh, 'path_to_directory_where_to_save_plot', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot softmax threshold simulation by class\n",
    "tools.helper.plot_accuracy_and_volume_vs_softmax_by_class(d_thresh_cl, 'path_to_directory_where_to_save_plot_by_class', cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
