{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import math\n",
    "\n",
    "import sys                              # only required for next line\n",
    "sys.path.append('/data/Simone/packages')  # add path to own package\n",
    "import tools                          # load own package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'name': 'Stoat',                                    # codename of the model - used for output path and filenames\n",
    "    'train_from': 'Stoat_V59',                        # either: 'imagenet' or full model name such as 'Stoat_V4'\n",
    "    'input_size': 299,\n",
    "    'pred_dataset': 'kiel_clean_5fold_000_valid',  # which dataset to predict\n",
    "    'col_cl_mapping': 'cl_15_w404',                  # which column mapping to use between real classes and target classes\n",
    "    'col_class': 'cl_manual',                           # column name in data .csv with class information\n",
    "    'col_image': 'filename' ,                            # column name in data .csv with filename of images (no path)\n",
    "    'col_id': 'defect_id'\n",
    "}\n",
    "\n",
    "path = {\n",
    "    'models': '/data/Simone/models/',                                # directory where models are saved\n",
    "\n",
    "    #'csv_train': '/data/DefectDensity/ostsee/kiel/kiel_clean_5fold_000_train.csv',   # path to .csv with training data (to be used with flow_from_dataframe Keras generator)\n",
    "    #'csv_valid': '/data/DefectDensity/ostsee/kiel/kiel_clean_5fold_000_valid.csv',   # path to .csv with validation data (to be used with flow_from_dataframe Keras generator)\n",
    "    #'csv_predict': '/data/DefectDensity/ostsee/kiel/kiel_clean_5fold_000_valid.csv', # path to csv or excel with data frame to images\n",
    "    'csv_meta': '/data/DefectDensity/ostsee/kiel_clean.xlsx',                            # path to csv or excel with metadata to enrich predictions\n",
    "    'csv_context': '/data/DefectDensity/contextdata/kiel_contextdata.csv',\n",
    "    \n",
    "    'data_train':'/images/kiel/' ,                              # path to image directory\n",
    "    'data_predict': '/images/kiel/',                            # path to image directory\n",
    "\n",
    "    'arch_overview': '/data/yury/ybtools/models/model_architectures.xlsx', # path to excel with architecture overview\n",
    "    'class_map': '/data/DefectDensity/DefectClassMap_Metal.xlsx'    # path to excel with mapping of classes between\n",
    "}\n",
    "\n",
    "hyper = {\n",
    "    'n_epochs_max': 50,                # maximum number of epochs for training, only reached if early stopping doesn't trigger\n",
    "    'lr_min': 1e-5,                     # minimum value for lr search\n",
    "    'lr_max': 1e-0,                     # maximum value for lr search\n",
    "    'n_lr_search': 100,                 # how many steps to take in the lr search space (exponential by default)\n",
    "    'lr_decay': 0,                      # learing rate decay parameter, suggested to use 0 if lr reduction callback is used\n",
    "    'patience_stop': 10,                # epochs without improvement to wait for early stopping. 20 is better for \"final\" models\n",
    "    'patience_lr': 6,                   # epochs without improvement to wait until learning rate reduction is triggered, 2 reductions within patience_stop occur\n",
    "    'lr_factor': 0.33,                  # factor by which learning rate is multiplied when lr reduction is triggered. good experience with factor 3 -> 2 reductions lead to 1 order of magnitude\n",
    "    'save_best_epoch_only': True,       # flag if only best epoch is saved or all epochs, default True\n",
    "    'dropout': False,                   # flag if Mustilidae dropout layer should be included or not. Not implemented for Beaver\n",
    "    'n_threshold_steps': 50,            # number of steps for softmax threshold simulation (O(n)) - default 20 equals 5% steps\n",
    "    'target_acc': 0.8,                  # target accuracy for volume simulation\n",
    "    'keras_preprocessing': True,        # use Keras preprocessing function or rescaling instead\n",
    "    'tensorboard': 'none',              # write output files for tensorboard. Possible values: 'minimal', 'full' and 'none' (or anything else)\n",
    "    'remove_to_404': False,             # Flag if remove-class images are actually supposed to be removed from prediction analysis. Does not influence training where remove is always removed\n",
    "    'meta_columns': [\"tech\",\"route\",\"op\",\"tool\"],  # which columns to add from csv_meta to enriched predictions                \n",
    "    'context_columns': [\"x(wafer)[um]\",\"y(wafer)[um]\",\"x(die)[um]\",\"y(die)[um]\",\"die_x\",\"die_y\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE SOME HYPER-PARAMETERS FROM ARCHITECTURE \n",
    "d = pd.read_excel(path['arch_overview'], sheet_name=0)\n",
    "hyper['batch_size'] = d.loc[d['codename']==cfg['name'],'batch_size'].values[0]\n",
    "hyper['threshold_layer'] = d.loc[d['codename']==cfg['name'],'threshold_layer'].values[0]\n",
    "hyper['lr_fb'] = d.loc[d['codename']==cfg['name'],'default_lr_fb'].values[0] # default learning rate if lr search is not used for frozen base model\n",
    "hyper['lr_pt'] = d.loc[d['codename']==cfg['name'],'default_lr_pt'].values[0] # default learning rate if lr search is not used for pretrained top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_names = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','U']\n",
    "n_classes = len(cl_names)\n",
    "\n",
    "d_map = pd.read_excel(path['class_map'],dtype=str)\n",
    "d_train = tools.helper.load_dataframe(path['csv_train'],cfg,d_map)\n",
    "d_valid = tools.helper.load_dataframe(path['csv_valid'],cfg,d_map)\n",
    "d_test = tools.helper.load_dataframe(path['csv_test'],cfg,d_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE TRAIN AND VALID DATA SUCH THAT IT CAN BE FED TO DATA-GENERATOR\n",
    "train_labels,train_filenames = tools.helper.prepare_data_only_images(d_train,hyper)\n",
    "valid_labels,valid_filenames = tools.helper.prepare_data_only_images(d_valid,hyper)\n",
    "test_labels,test_filenames = tools.helper.prepare_data_only_images(d_test,hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TRAIN AND VALID DATA-GENERATOR\n",
    "train_datagen = tools.helper.MyImageDataGenerator(img_files=train_filenames,labels=train_labels, batch_size=hyper['batch_size'] , n_classes=n_classes)\n",
    "val_datagen = tools.helper.MyImageDataGenerator(img_files=valid_filenames,labels=valid_labels, batch_size=hyper['batch_size'], n_classes=n_classes,shuffle=False)\n",
    "test_datagen = tools.helper.MyImageDataGenerator(img_files=test_filenames,labels=test_labels, batch_size=hyper['batch_size'],n_classes=n_classes,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cl = tools.helper.summarize_train_valid_test_data(d_train,d_valid,d_test,cl_names)\n",
    "# dictionary with weight for loss function adjustment\n",
    "cl_weight_dict = dict(zip(d_cl['cl_id'], d_cl['inv_ratio_adj_train'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED MODEL\n",
    "m = tf.keras.models.load_model('/data/Simone/models/Stoat_V55.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESIZE LAST LAYER\n",
    "\n",
    "# creates new model without last layer\n",
    "m_base = tf.keras.models.Model(inputs=m.input, outputs=m.layers[-2].output) \n",
    "# add classification layer    \n",
    "classification_layer = tf.keras.layers.Dense(n_classes, activation='softmax', name='classification')(m_base.output)\n",
    "m_images = tf.keras.models.Model(inputs=m_base.input, outputs=classification_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_images.compile(optimizer=tf.keras.optimizers.Adam(lr=hyper['lr_pt'], decay=hyper['lr_decay']),\n",
    "              loss=WeightedCategoricalCrossentropy(cost_matrix),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive some hyper-parameters for training\n",
    "hyper['n_steps_train'] = int(np.ceil(train_datagen.__len__()/train_datagen.batch_size))\n",
    "hyper['n_steps_valid'] = int(np.ceil(val_datagen.__len__()/val_datagen.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding callbacks\n",
    "filepath = \"path_to_directory_where_to_save_best_model\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1,save_weights_only=False,save_best_only=True, mode='max')\n",
    "check_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',min_delta=0.0001,patience=hyper['patience_lr'],verbose=1)\n",
    "check_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=hyper['patience_stop'],verbose=1,restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,check_lr,check_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = m_images.fit_generator(train_datagen, \n",
    "                           steps_per_epoch=hyper['n_steps_train'], \n",
    "                           epochs=hyper['n_epochs_max'], \n",
    "                           class_weight = cl_weight_dict,\n",
    "                           callbacks = callbacks_list, \n",
    "                           validation_data= val_datagen, \n",
    "                           validation_steps=hyper['n_steps_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "tools.helper.model_history_analysis(model_trained, '/data/Simone/only_images/plots/model_history.png', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(filepath)\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(lr=hyper['lr_pt'],decay=hyper['lr_decay']),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = m_images.predict_generator(test_datagen, steps=len(test_datagen), verbose=1)\n",
    "predictions = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE NEW PREDICTIONS BY WEIGHTING OLD PREDICTIONS WITH BAYESIAN PRIORS\n",
    "new_probabilities = np.ones(shape=probabilities.shape)\n",
    "bayesian_priors = np.load('/data/Simone/predictions/bayesian_priors_1500.npy')\n",
    "weights_for_priors = np.load('/data/Simone/predictions/weights_for_priors.npy')\n",
    "\n",
    "for i in range(new_probabilities.shape[0]):\n",
    "  \n",
    "    weighted_bayesian_priors = np.multiply(bayesian_priors[i],weights_for_priors)\n",
    "    weighted_bayesian_priors = weighted_bayesian_priors/np.sum(weighted_bayesian_priors)\n",
    "\n",
    "    new_probabilities[i] = np.multiply(probabilities[i],weighted_bayesian_priors)\n",
    "    N = np.sum(new_probabilities[i])\n",
    "    new_probabilities[i] = new_probabilities[i]/N\n",
    "    \n",
    "new_predictions = np.argmax(new_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DERIVE TRUE LABELS\n",
    "test_values = np.array([t for t in test_labels.values()])\n",
    "test_values = test_values[0:probabilities.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "tools.helper.plot_confusion_matrix(test_values , predictions , n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classification score\n",
    "report = classification_report(test_values, new_predictions)\n",
    "report_df = tools.helper.report_to_df(report,'path_to_directory_where_to_save_classification_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns keras probabilities into a properly named pandas data frame\n",
    "classes = []\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    classes.append(tools.helper.assign_class(test_values[i]))\n",
    "\n",
    "d_softmax_2 = tools.helper.prepare_base_table(new_probabilities, cl_names,classes,test_filenames,'path_to_directory_where_to_save_softmax_vector2')\n",
    "d_softmax = tools.helper.prepare_base_table(probabilities, cl_names,classes,test_filenames,'path_to_directory_where_to_save_softmax_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax threshold analysis \n",
    "hyper['threshold_range'] = np.array(range(hyper['n_threshold_steps']))/hyper['n_threshold_steps']\n",
    "cfg['n_img'] = d_softmax.shape[0]\n",
    "d_thresh, d_thresh_cl = tools.helper.simulate_softmax_thresholds(d_softmax,cfg,hyper)\n",
    "d_thresh_2, d_thresh_cl_2 = tools.helper.simulate_softmax_thresholds(d_softmax_2,cfg,hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot softmax threshold simulation: comparison with/without priors\n",
    "tools.helper.plot_accuracy_and_volume_vs_softmax_2(d_thresh,d_thresh_2,'path_to_directory_where_to_save_plot_by_class',cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot softmax threshold simulation by class: comparison with/without priors\n",
    "tools.helper.plot_accuracy_and_volume_vs_softmax_by_class_2(d_thresh_cl,d_thresh_cl_2, 'path_to_directory_where_to_save_plot_by_class', cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
